{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7월4일 자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision.datasets as dsets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "mnist_train = dsets.MNIST(root=\"MNIST_data/\", train=True, transform=transforms.ToTensor(),download=True)\n",
    "mnist_test = dsets.MNIST(root=\"MNIST_data/\", train=False, transform=transforms.ToTensor(),download=True)\n",
    "#parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "linear = torch.nn.Linear(784, 10, bias=True)\n",
    "#initialization\n",
    "torch.nn.init.normal_(linear.weight)\n",
    "\n",
    "\n",
    "# define cost/Loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # Label is not one-hot encoded\n",
    "        X = X.view(-1,28 * 28)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost/ total_batch\n",
    "    print(\"Epoch: \", \"%04d\" %(epoch+1), \"cost = \",\"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Test the model using test sets\n",
    "    with torch.no_grad():\n",
    "        X_test = mnist_test.test_data.view(-1, 28*28).float()\n",
    "        Y_test = mnist_test.test_labels\n",
    "        \n",
    "        prediction = linear(X_test)\n",
    "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        accuracy = correct_prediction.float().mean()\n",
    "        print(\"Accuracy: \", accuracy.item())\n",
    "\n",
    "r = random.randint(0, len(mnist_test)-1)\n",
    "X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float()\n",
    "Y_single_data = mnist_test.test_labels[r:r+1]\n",
    "\n",
    "print(\"Label : \", Y_single_data.item())\n",
    "single_prediction = linear(X_single_data)\n",
    "print(\"Prediction: \",torch.argmax(single_prediction, 1).item())\n",
    "\n",
    "plt.imshow(mnist_test.test_data[r:r+1].view(28,28),cmap=\"Greys\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7월5일 자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "#MNIST data image of shape 28 * 28 784\n",
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "#Initialization\n",
    "torch.nn.init.normal_(linear1.weight)\n",
    "torch.nn.init.normal_(linear2.weight)\n",
    "torch.nn.init.normal_(linear3.weight)\n",
    "\n",
    "#model\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3)\n",
    "\n",
    "#define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost = 750.499023438\n",
      "Epoch:  0002 cost = 785.879638672\n",
      "Epoch:  0003 cost = 806.223754883\n",
      "Epoch:  0004 cost = 818.138427734\n",
      "Epoch:  0005 cost = 822.149658203\n",
      "Epoch:  0006 cost = 824.509094238\n",
      "Epoch:  0007 cost = 826.694641113\n",
      "Epoch:  0008 cost = 828.835266113\n",
      "Epoch:  0009 cost = 830.962463379\n",
      "Epoch:  0010 cost = 833.078613281\n",
      "Epoch:  0011 cost = 835.192321777\n",
      "Epoch:  0012 cost = 837.308532715\n",
      "Epoch:  0013 cost = 839.477416992\n",
      "Epoch:  0014 cost = 841.651489258\n",
      "Epoch:  0015 cost = 843.671691895\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision.datasets as dsets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "mnist_train = dsets.MNIST(root=\"MNIST_data/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = dsets.MNIST(root=\"MNIST_data/\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "\n",
    "#MNIST data image of shape 28 * 28 784\n",
    "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
    "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
    "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "#Initialization\n",
    "torch.nn.init.normal_(linear1.weight)\n",
    "torch.nn.init.normal_(linear2.weight)\n",
    "torch.nn.init.normal_(linear3.weight)\n",
    "#model\n",
    "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3)\n",
    "      \n",
    "#define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    step=0\n",
    "    for X,Y in data_loader:\n",
    "        #reshape input image into [batch_size by 784]\n",
    "        #Label is not one-hot encoded\n",
    "        X = X.view(-1,28 * 28)\n",
    "        \n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "        \n",
    "        step=step+1\n",
    "        \n",
    "      \n",
    "    print(\"Epoch: \", \"%04d\" % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision.datasets as dsets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#for reproducibility\n",
    "#torch.manual_seed(777)\n",
    "#if device == 'cuda':\n",
    "#  torch.cuda.manual_seed_all(777)\n",
    "\n",
    "#parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 15\n",
    "batch_size = 10\n",
    "\n",
    "mnist_train = dsets.MNIST(root=\"MNIST_data/\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = dsets.MNIST(root=\"MNIST_data/\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_test, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "\n",
    "#MNIST data image of shape 28 * 28 784\n",
    "linear1 = torch.nn.Linear(784, 512, bias=True)\n",
    "linear2 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear3 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear4 = torch.nn.Linear(512, 512, bias=True)\n",
    "linear5 = torch.nn.Linear(512, 10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "#Initialization\n",
    "torch.nn.init.normal_(linear1.weight)\n",
    "torch.nn.init.normal_(linear2.weight)\n",
    "torch.nn.init.normal_(linear3.weight)\n",
    "torch.nn.init.normal_(linear4.weight)\n",
    "torch.nn.init.normal_(linear5.weight)\n",
    "#model\n",
    "model = torch.nn.Sequential(linear1, relu,dropout, linear2, relu, linear3,relu, linear4, relu,dropout, linear5)\n",
    "      \n",
    "#define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    step=0\n",
    "    for X,Y in data_loader:\n",
    "        #reshape input image into [batch_size by 784]\n",
    "        #Label is not one-hot encoded\n",
    "        X = X.view(-1,28 * 28)\n",
    "        \n",
    "        hypothesis = model(X)\n",
    "        \n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "        \n",
    "        step=step+1\n",
    "        \n",
    "      \n",
    "    print(\"Epoch: \", \"%04d\" % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 예시문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as dsets\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "mnist_train = dsets.MNIST(root=\"MNIST_data/\", train=True, transform=transforms.ToTensor(),download=True)\n",
    "print(mnist_train)\n",
    "mnist_test = dsets.MNIST(root=\"MNIST_data/\", train=False, transform=transforms.ToTensor(),download=True)\n",
    "training_epochs = 1\n",
    "batch_size = 100\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "linear = torch.nn.Linear(784, 10, bias=True).to(device)\n",
    "\n",
    "torch.nn.init.normal_(linear.weight)\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "# model=torch.nn.Sequential(linear,sigmoid)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print(epoch,\" : \", avg_cost)\n",
    "    with torch.no_grad():\n",
    "        X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "        Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "\n",
    "        prediction = linear(X_test)\n",
    "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        accuracy = correct_prediction.float().mean()\n",
    "        print(\"Accuracy\", accuracy.item())\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test=mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "    Y_test=mnist_test.test_labels.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    prediction=linear(X_test)\n",
    "    correct_prediction=torch.argmax(prediction,1)==Y_test\n",
    "    accuracy=correct_prediction.float().mean()\n",
    "    print(\"Accuracy\", accuracy.item())\n",
    "\n",
    "    # visualization\n",
    "\n",
    "    r=random.randint(0,len(mnist_test)-1)\n",
    "    X_single_data=mnist_test.test_data[r:r+5].view(-1,28*28).float().to(device)\n",
    "    Y_single_data=mnist_test.test_labels[r:r+5].to(device)\n",
    "\n",
    "    print(\"Label: \", Y_single_data)\n",
    "    single_prediction=linear(X_single_data)\n",
    "    print(\"Prediction_softmax: \",F.softmax(single_prediction))\n",
    "    print(\"Prediction: \", torch.argmax(single_prediction,1))\n",
    "\n",
    "    fig, (ax0, ax1,ax2,ax3,ax4) = plt.subplots(1, 5)\n",
    "    ax0.imshow(mnist_test.test_data[r:r+1].view(28,28), cmap='gray')\n",
    "    ax1.imshow(mnist_test.test_data[r+1:r+2].view(28,28), cmap='gray');\n",
    "    ax2.imshow(mnist_test.test_data[r+2:r+3].view(28,28), cmap='gray')\n",
    "    ax3.imshow(mnist_test.test_data[r+3:r+4].view(28,28), cmap='gray');\n",
    "    ax4.imshow(mnist_test.test_data[r+4:r+ 5].view(28, 28), cmap='gray');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
