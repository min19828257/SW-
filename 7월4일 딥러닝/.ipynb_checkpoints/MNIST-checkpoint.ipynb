{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x64c4af0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1664, 0.1871, 0.1737, 0.2695, 0.2033],\n",
      "        [0.2002, 0.1783, 0.2218, 0.1944, 0.2054],\n",
      "        [0.1809, 0.2380, 0.2318, 0.1084, 0.2409]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5,(3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1,y.unsqueeze(1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6800, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot*-torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1,2,1],\n",
    "                            [1,3,2],\n",
    "                            [1,3,4],\n",
    "                            [1,5,5],\n",
    "                            [1,7,5],\n",
    "                            [1,2,5],\n",
    "                            [1,6,6],\n",
    "                            [1,7,7]         \n",
    "                            ])\n",
    "y_train = torch.LongTensor([2,2,2,1,1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.FloatTensor([[2,1,1],[3,1,2],[3,3,4]])\n",
    "y_test = torch.LongTensor([2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,3)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        #H(x) 계산\n",
    "        predictioin = model(x_train)\n",
    "        \n",
    "        #cost 계산\n",
    "        cost = F.cross_entropy(prediction, y_train)\n",
    "        \n",
    "        #cost로 H(x)로 개선\n",
    "        optimizer.zero_grad\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73,80,75],\n",
    "                             [93,88,93],\n",
    "                             [89,91,90],\n",
    "                             [96,98,100],\n",
    "                             [73,66,70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리( 표준편차는 1로 만든다. 그리고 입력값은 0~1사이로 지정한다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0674, -0.3758, -0.8398],\n",
      "        [ 0.7418,  0.2778,  0.5863],\n",
      "        [ 0.3799,  0.5229,  0.3486],\n",
      "        [ 1.0132,  1.0948,  1.1409],\n",
      "        [-1.0674, -1.5197, -1.2360]])\n"
     ]
    }
   ],
   "source": [
    "mu = x_train.mean(dim=0) #첫번째 세로줄을 의 평균을 구함 (X좌표의 평균을 구함)\n",
    "sigma = x_train.std(dim=0) #표준편차를 구함\n",
    "norm_x_train = (x_train - mu) / sigma\n",
    "print(norm_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 처리(정규화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 200\n",
    "    for epoch in range(nb_epochs):\n",
    "        \n",
    "        #H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        #cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        #cost로 H(x) 개선\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 알고리즘 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/200 Cost: 0.104585\n",
      "Epoch    1/200 Cost: 0.104584\n",
      "Epoch    2/200 Cost: 0.104585\n",
      "Epoch    3/200 Cost: 0.104583\n",
      "Epoch    4/200 Cost: 0.104584\n",
      "Epoch    5/200 Cost: 0.104584\n",
      "Epoch    6/200 Cost: 0.104584\n",
      "Epoch    7/200 Cost: 0.104582\n",
      "Epoch    8/200 Cost: 0.104583\n",
      "Epoch    9/200 Cost: 0.104581\n",
      "Epoch   10/200 Cost: 0.104582\n",
      "Epoch   11/200 Cost: 0.104582\n",
      "Epoch   12/200 Cost: 0.104582\n",
      "Epoch   13/200 Cost: 0.104578\n",
      "Epoch   14/200 Cost: 0.104583\n",
      "Epoch   15/200 Cost: 0.104581\n",
      "Epoch   16/200 Cost: 0.104581\n",
      "Epoch   17/200 Cost: 0.104582\n",
      "Epoch   18/200 Cost: 0.104580\n",
      "Epoch   19/200 Cost: 0.104581\n",
      "Epoch   20/200 Cost: 0.104581\n",
      "Epoch   21/200 Cost: 0.104581\n",
      "Epoch   22/200 Cost: 0.104579\n",
      "Epoch   23/200 Cost: 0.104579\n",
      "Epoch   24/200 Cost: 0.104580\n",
      "Epoch   25/200 Cost: 0.104580\n",
      "Epoch   26/200 Cost: 0.104579\n",
      "Epoch   27/200 Cost: 0.104580\n",
      "Epoch   28/200 Cost: 0.104580\n",
      "Epoch   29/200 Cost: 0.104578\n",
      "Epoch   30/200 Cost: 0.104578\n",
      "Epoch   31/200 Cost: 0.104578\n",
      "Epoch   32/200 Cost: 0.104577\n",
      "Epoch   33/200 Cost: 0.104577\n",
      "Epoch   34/200 Cost: 0.104580\n",
      "Epoch   35/200 Cost: 0.104579\n",
      "Epoch   36/200 Cost: 0.104579\n",
      "Epoch   37/200 Cost: 0.104579\n",
      "Epoch   38/200 Cost: 0.104577\n",
      "Epoch   39/200 Cost: 0.104579\n",
      "Epoch   40/200 Cost: 0.104575\n",
      "Epoch   41/200 Cost: 0.104575\n",
      "Epoch   42/200 Cost: 0.104575\n",
      "Epoch   43/200 Cost: 0.104574\n",
      "Epoch   44/200 Cost: 0.104577\n",
      "Epoch   45/200 Cost: 0.104578\n",
      "Epoch   46/200 Cost: 0.104576\n",
      "Epoch   47/200 Cost: 0.104578\n",
      "Epoch   48/200 Cost: 0.104576\n",
      "Epoch   49/200 Cost: 0.104575\n",
      "Epoch   50/200 Cost: 0.104574\n",
      "Epoch   51/200 Cost: 0.104575\n",
      "Epoch   52/200 Cost: 0.104573\n",
      "Epoch   53/200 Cost: 0.104574\n",
      "Epoch   54/200 Cost: 0.104574\n",
      "Epoch   55/200 Cost: 0.104577\n",
      "Epoch   56/200 Cost: 0.104575\n",
      "Epoch   57/200 Cost: 0.104573\n",
      "Epoch   58/200 Cost: 0.104574\n",
      "Epoch   59/200 Cost: 0.104573\n",
      "Epoch   60/200 Cost: 0.104571\n",
      "Epoch   61/200 Cost: 0.104574\n",
      "Epoch   62/200 Cost: 0.104572\n",
      "Epoch   63/200 Cost: 0.104571\n",
      "Epoch   64/200 Cost: 0.104574\n",
      "Epoch   65/200 Cost: 0.104574\n",
      "Epoch   66/200 Cost: 0.104575\n",
      "Epoch   67/200 Cost: 0.104573\n",
      "Epoch   68/200 Cost: 0.104570\n",
      "Epoch   69/200 Cost: 0.104573\n",
      "Epoch   70/200 Cost: 0.104572\n",
      "Epoch   71/200 Cost: 0.104570\n",
      "Epoch   72/200 Cost: 0.104570\n",
      "Epoch   73/200 Cost: 0.104570\n",
      "Epoch   74/200 Cost: 0.104571\n",
      "Epoch   75/200 Cost: 0.104571\n",
      "Epoch   76/200 Cost: 0.104569\n",
      "Epoch   77/200 Cost: 0.104571\n",
      "Epoch   78/200 Cost: 0.104569\n",
      "Epoch   79/200 Cost: 0.104570\n",
      "Epoch   80/200 Cost: 0.104570\n",
      "Epoch   81/200 Cost: 0.104569\n",
      "Epoch   82/200 Cost: 0.104569\n",
      "Epoch   83/200 Cost: 0.104569\n",
      "Epoch   84/200 Cost: 0.104568\n",
      "Epoch   85/200 Cost: 0.104570\n",
      "Epoch   86/200 Cost: 0.104569\n",
      "Epoch   87/200 Cost: 0.104569\n",
      "Epoch   88/200 Cost: 0.104569\n",
      "Epoch   89/200 Cost: 0.104568\n",
      "Epoch   90/200 Cost: 0.104568\n",
      "Epoch   91/200 Cost: 0.104568\n",
      "Epoch   92/200 Cost: 0.104566\n",
      "Epoch   93/200 Cost: 0.104566\n",
      "Epoch   94/200 Cost: 0.104564\n",
      "Epoch   95/200 Cost: 0.104564\n",
      "Epoch   96/200 Cost: 0.104568\n",
      "Epoch   97/200 Cost: 0.104568\n",
      "Epoch   98/200 Cost: 0.104567\n",
      "Epoch   99/200 Cost: 0.104567\n",
      "Epoch  100/200 Cost: 0.104565\n",
      "Epoch  101/200 Cost: 0.104567\n",
      "Epoch  102/200 Cost: 0.104566\n",
      "Epoch  103/200 Cost: 0.104564\n",
      "Epoch  104/200 Cost: 0.104564\n",
      "Epoch  105/200 Cost: 0.104564\n",
      "Epoch  106/200 Cost: 0.104568\n",
      "Epoch  107/200 Cost: 0.104566\n",
      "Epoch  108/200 Cost: 0.104566\n",
      "Epoch  109/200 Cost: 0.104566\n",
      "Epoch  110/200 Cost: 0.104566\n",
      "Epoch  111/200 Cost: 0.104562\n",
      "Epoch  112/200 Cost: 0.104564\n",
      "Epoch  113/200 Cost: 0.104562\n",
      "Epoch  114/200 Cost: 0.104563\n",
      "Epoch  115/200 Cost: 0.104563\n",
      "Epoch  116/200 Cost: 0.104561\n",
      "Epoch  117/200 Cost: 0.104565\n",
      "Epoch  118/200 Cost: 0.104565\n",
      "Epoch  119/200 Cost: 0.104564\n",
      "Epoch  120/200 Cost: 0.104563\n",
      "Epoch  121/200 Cost: 0.104564\n",
      "Epoch  122/200 Cost: 0.104562\n",
      "Epoch  123/200 Cost: 0.104562\n",
      "Epoch  124/200 Cost: 0.104560\n",
      "Epoch  125/200 Cost: 0.104561\n",
      "Epoch  126/200 Cost: 0.104561\n",
      "Epoch  127/200 Cost: 0.104562\n",
      "Epoch  128/200 Cost: 0.104562\n",
      "Epoch  129/200 Cost: 0.104560\n",
      "Epoch  130/200 Cost: 0.104558\n",
      "Epoch  131/200 Cost: 0.104561\n",
      "Epoch  132/200 Cost: 0.104559\n",
      "Epoch  133/200 Cost: 0.104559\n",
      "Epoch  134/200 Cost: 0.104559\n",
      "Epoch  135/200 Cost: 0.104560\n",
      "Epoch  136/200 Cost: 0.104558\n",
      "Epoch  137/200 Cost: 0.104558\n",
      "Epoch  138/200 Cost: 0.104562\n",
      "Epoch  139/200 Cost: 0.104562\n",
      "Epoch  140/200 Cost: 0.104560\n",
      "Epoch  141/200 Cost: 0.104562\n",
      "Epoch  142/200 Cost: 0.104558\n",
      "Epoch  143/200 Cost: 0.104559\n",
      "Epoch  144/200 Cost: 0.104558\n",
      "Epoch  145/200 Cost: 0.104557\n",
      "Epoch  146/200 Cost: 0.104557\n",
      "Epoch  147/200 Cost: 0.104557\n",
      "Epoch  148/200 Cost: 0.104559\n",
      "Epoch  149/200 Cost: 0.104559\n",
      "Epoch  150/200 Cost: 0.104557\n",
      "Epoch  151/200 Cost: 0.104557\n",
      "Epoch  152/200 Cost: 0.104557\n",
      "Epoch  153/200 Cost: 0.104556\n",
      "Epoch  154/200 Cost: 0.104556\n",
      "Epoch  155/200 Cost: 0.104556\n",
      "Epoch  156/200 Cost: 0.104556\n",
      "Epoch  157/200 Cost: 0.104556\n",
      "Epoch  158/200 Cost: 0.104558\n",
      "Epoch  159/200 Cost: 0.104560\n",
      "Epoch  160/200 Cost: 0.104556\n",
      "Epoch  161/200 Cost: 0.104558\n",
      "Epoch  162/200 Cost: 0.104556\n",
      "Epoch  163/200 Cost: 0.104555\n",
      "Epoch  164/200 Cost: 0.104555\n",
      "Epoch  165/200 Cost: 0.104553\n",
      "Epoch  166/200 Cost: 0.104553\n",
      "Epoch  167/200 Cost: 0.104553\n",
      "Epoch  168/200 Cost: 0.104552\n",
      "Epoch  169/200 Cost: 0.104555\n",
      "Epoch  170/200 Cost: 0.104555\n",
      "Epoch  171/200 Cost: 0.104556\n",
      "Epoch  172/200 Cost: 0.104555\n",
      "Epoch  173/200 Cost: 0.104553\n",
      "Epoch  174/200 Cost: 0.104554\n",
      "Epoch  175/200 Cost: 0.104554\n",
      "Epoch  176/200 Cost: 0.104553\n",
      "Epoch  177/200 Cost: 0.104553\n",
      "Epoch  178/200 Cost: 0.104553\n",
      "Epoch  179/200 Cost: 0.104551\n",
      "Epoch  180/200 Cost: 0.104551\n",
      "Epoch  181/200 Cost: 0.104552\n",
      "Epoch  182/200 Cost: 0.104553\n",
      "Epoch  183/200 Cost: 0.104552\n",
      "Epoch  184/200 Cost: 0.104553\n",
      "Epoch  185/200 Cost: 0.104553\n",
      "Epoch  186/200 Cost: 0.104552\n",
      "Epoch  187/200 Cost: 0.104550\n",
      "Epoch  188/200 Cost: 0.104550\n",
      "Epoch  189/200 Cost: 0.104548\n",
      "Epoch  190/200 Cost: 0.104551\n",
      "Epoch  191/200 Cost: 0.104551\n",
      "Epoch  192/200 Cost: 0.104552\n",
      "Epoch  193/200 Cost: 0.104552\n",
      "Epoch  194/200 Cost: 0.104553\n",
      "Epoch  195/200 Cost: 0.104551\n",
      "Epoch  196/200 Cost: 0.104551\n",
      "Epoch  197/200 Cost: 0.104551\n",
      "Epoch  198/200 Cost: 0.104547\n",
      "Epoch  199/200 Cost: 0.104547\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, norm_x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchvision 설치법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dsets\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost =  2.727510929\n",
      "Accuracy:  0.7504000067710876\n",
      "Epoch:  0002 cost =  1.114668369\n",
      "Accuracy:  0.8131999969482422\n",
      "Epoch:  0003 cost =  0.883005321\n",
      "Accuracy:  0.8385000228881836\n",
      "Epoch:  0004 cost =  0.768246233\n",
      "Accuracy:  0.8463000059127808\n",
      "Epoch:  0005 cost =  0.696319759\n",
      "Accuracy:  0.8571000099182129\n",
      "Epoch:  0006 cost =  0.645372570\n",
      "Accuracy:  0.8618999719619751\n",
      "Epoch:  0007 cost =  0.606003225\n",
      "Accuracy:  0.8669999837875366\n",
      "Epoch:  0008 cost =  0.575463653\n",
      "Accuracy:  0.8702999949455261\n",
      "Epoch:  0009 cost =  0.550106943\n",
      "Accuracy:  0.8715999722480774\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision.datasets as dsets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "mnist_train = dsets.MNIST(root=\"MNIST_data/\", train=True, transform=transforms.ToTensor(),download=True)\n",
    "mnist_test = dsets.MNIST(root=\"MNIST_data/\", train=False, transform=transforms.ToTensor(),download=True)\n",
    "#parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size,shuffle=True, drop_last=True)\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "linear = torch.nn.Linear(784, 10, bias=True)\n",
    "#initialization\n",
    "torch.nn.init.normal_(linear.weight)\n",
    "\n",
    "\n",
    "# define cost/Loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    for X, Y in data_loader:\n",
    "        # reshape input image into [batch_size by 784]\n",
    "        # Label is not one-hot encoded\n",
    "        X = X.view(-1,28 * 28)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost/ total_batch\n",
    "    print(\"Epoch: \", \"%04d\" %(epoch+1), \"cost = \",\"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Test the model using test sets\n",
    "    with torch.no_grad():\n",
    "        X_test = mnist_test.test_data.view(-1, 28*28).float()\n",
    "        Y_test = mnist_test.test_labels\n",
    "        \n",
    "        prediction = linear(X_test)\n",
    "        correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "        accuracy = correct_prediction.float().mean()\n",
    "        print(\"Accuracy: \", accuracy.item())\n",
    "\n",
    "r = random.randint(0, len(mnist_test)-1)\n",
    "X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float()\n",
    "Y_single_data = mnist_test.test_labels[r:r+1]\n",
    "\n",
    "print(\"Label : \", Y_single_data.item())\n",
    "single_prediction = linear(X_single_data)\n",
    "print(\"Prediction: \",torch.argmax(single_prediction, 1).item())\n",
    "\n",
    "plt.imshow(mnist_test.test_data[r:r+1].view(28,28),cmap=\"Greys\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
